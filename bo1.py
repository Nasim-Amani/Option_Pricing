# -*- coding: utf-8 -*-
"""BO1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MJR9ddjME9T8jhvyyHH5B8aNJScz3taN
"""



# ✅ Define a custom HyperModel class properly
class LSTMHyperModel(kt.HyperModel):
    def build(self, hp):
        """
        Build an LSTM model with tunable hyperparameters.

        Parameters:
        - hp: Hyperparameter object from Keras Tuner

        Returns:
        - Compiled LSTM model
        """
        model = Sequential()

        # ✅ Determine the number of LSTM layers
        num_layers = hp.Int('num_layers', min_value=1, max_value=3)

        for i in range(num_layers):
            units = hp.Int(f'units_{i}', min_value=16, max_value=256, step=16)
            dropout_rate = hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.05)

            return_sequences = True if i < num_layers - 1 else False  # Last layer should not return sequences

            model.add(LSTM(units=units, return_sequences=return_sequences,
                           input_shape=(timesteps, features) if i == 0 else None))
            model.add(Dropout(rate=dropout_rate))

        # ✅ Output layer
        model.add(Dense(units=1))

        # ✅ Define learning rate as a hyperparameter
        initial_learning_rate = hp.Float('initial_learning_rate', 1e-5, 5e-3, sampling='log')
        optimizer = Adam(learning_rate=initial_learning_rate)

        model.compile(optimizer=optimizer, loss="mean_squared_error")
        batch_size=hp.Choice("batch_size", [16, 32, 64, 128]),  # ✅ Retrieves batch size correctly
        return model

    def fit(self, hp, model, *args, **kwargs):
        """
        Custom fit function to ensure batch size is tunable.
        """
        return model.fit(
            *args,

            **kwargs,
        )

# ✅ Define Early Stopping & Learning Rate Scheduler
early_stopping = EarlyStopping(
    monitor='val_loss', patience=5, restore_best_weights=True, verbose=1
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss', patience=3, verbose=1
)

# ✅ Initialize the HyperModel properly
print("🔍 Running Bayesian Optimization ")
hypermodel = LSTMHyperModel()

# ✅ Bayesian Optimization with Corrected HyperModel
tuner = kt.BayesianOptimization(
    hypermodel,
    objective='val_loss',  # Minimize MSE
    max_trials=20,  # Adjust the number of search trials
    executions_per_trial=1,
    directory='lstm_tuning',
    project_name='bayesian_lstm'
)

# ✅ Start the hyperparameter search
tuner.search(
    X_train_scaled, y_train_scaled,
    validation_data=(X_val_scaled, y_val_scaled),
    epochs=100,  # Reduce to 50 for faster tuning
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)